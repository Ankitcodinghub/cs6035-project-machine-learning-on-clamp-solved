# cs6035-project-machine-learning-on-clamp-solved
**TO GET THIS SOLUTION VISIT:** [CS6035 Project Machine Learning on CLAMP Solved](https://www.ankitcodinghub.com/product/cs6035-project-machine-learning-on-clamp-solved/)


---

üì© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
üì± **WhatsApp:** +1 419 877 7882  
üìÑ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;102186&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;3&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;5&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;5\/5 - (3 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;CS6035 Project  Machine Learning on CLAMP Solved&quot;,&quot;width&quot;:&quot;138&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 138px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            5/5 - (3 votes)    </div>
    </div>
<h2></h2>
<h4><a href="https://www.ankitcodinghub.com/product/cs6035-machine-learning-2025-solved/"><span style="color: #0000ff;"><strong>SPRING 2025 SOLUTION LINK</strong></span></a></h4>
<h4><a href="https://www.ankitcodinghub.com/product/cs-6035-projects-machine-learning-2024-fall-solved/"><strong><span style="color: #0000ff;">FALL2024 Solution VIEW HERE</span></strong></a></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/classes.html"><strong>API Reference ‚Äî scikit-learn 1.1.2 documentation</strong></a></li>
<li><a href="https://www.kdnuggets.com/2016/03/data-science-process.html"><strong>https://www.kdnuggets.com/2016/03/data-science-process.html</strong></a></li>
<li><a href="https://pandas.pydata.org/docs/getting_started/index.html#getting-started"><strong>Getting started ‚Äî pandas 1.4.3 documentation (pydata.org)</strong></a></li>
<li><a href="https://elitedatascience.com/python-cheat-sheet"><strong>Python Cheat Sheet for Data Science (elitedatascience.com)</strong></a></li>
</ul>
&nbsp;

<strong>&nbsp;</strong>

<h1><strong><u>Introduction</u></strong></h1>
You may be asking yourself ‚Äúwhat is the importance of learning about Data Science and Machine Learning in a cybersecurity class?‚Äù, The short answer is that data science is a useful set of tools to handle the massive amount of data that flow through IT systems and it is used by many security teams either explicitly or within tools/programs they use so it is important to get a basic understanding of how it works. This Project will go through a simplified scenario where data science can be used, if this sparks your interest there are plenty of other ML focused classes at GaTech that you may be interested in taking, as well as a wealth of training materials on Youtube, Coursera, Udacity, Udemy, DataCamp etc that you could use to go deeper into the field.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

<h1><strong><u>Scenario:</u></strong></h1>
You are an analyst on a security team for a midsized software company that runs a messaging app (a slack, gchat, microsoft teams competitor). It is Monday morning and you see an email from your manager setting up a meeting to discuss a new security feature that needs to be implemented in the product ASAP. You join the meeting and learn that recently there has been a big uptick in malicious executable files being sent over the chat app and it is starting to generate bad press for the company. A few analysts on the team already worked on analyzing a set of files sent over the app and classifying them as malicious or benign. They also used a python library (pefile) to get some attributes of each executable file and have created a CSV with those extracted attributes and a column with the name <strong>class</strong> with a 1 denoting a malicious file and a 0 denoting a benign files. They documented their preprocessing work in a readme in the git repo (<a href="https://github.com/urwithajit9/ClaMP">urwithajit9/ClaMP: A Malware classifier dataset built with header fields‚Äô values of Portable Executable files (github.com)</a>) and shared the repo with software engineers so they can get to work writing code that will generate those features for every executable file sent over the messaging app. Your boss turns to you and says I would like you to help us to understand a bit more about how big of a problem this is on our app and write a model that takes in these features and produces a propensity score from 0 to 1 where scores closer to 0 mean a low likelihood of the file being malicious and closer to a 1 means a higher likelihood of a file being malicious. Also since the team may want to reuse this type of work in the future for different types of files or with different extracted attributes you should create functions that can be used in the future with minimal rework. Once you produce a model, you will share your code and the trained model file with the software engineers who will integrate the model into the messaging app and will score all files uploaded to the app.

&nbsp;

&nbsp;

&nbsp;

<h1><strong><u>General Advice</u></strong></h1>
<ul>
<li>Develop locally then test in the autograder when you are confident your code runs without errors. You can run the python files locally, develop in a local vscode/jupyter notebook or on a hosted web notebook like google colab.</li>
<li>Do <strong>not </strong>use print statements in your gradescope submissions. While print statements are useful to debug issues locally in an autograder context they can leak sensitive answer information. We have detections in gradescope that will block you from viewing scores/outputs from your code if you use print statements in any of your submitted code. If you try to bruteforce/hack/game the autograder or extract information we will give you a 0 for the whole assignment.</li>
<li>Read the python library documentation. You will be using pandas, scikit-learn, yellowbrick</li>
<li>Do not hard code solutions (see screenshot below for what not to do):</li>
</ul>
&nbsp;

&nbsp;

<h1><strong><u>Task 0 (20 points)</u></strong></h1>
We have a Canvas quiz that is meant to test that you have read the library documentation for the packages we use for this class. It is not meant to be tricky and can be completed before you start the project or after you finish it.

&nbsp;

<em>Useful Links:</em>

<ul>
<li><a href="https://scikit-learn.org/stable/">scikit-learn Documentation</a></li>
</ul>
&nbsp;

<em>Deliverables:</em>

<ul>
<li>Complete Canvas Quiz</li>
</ul>
&nbsp;

<h1><strong><u>Task 1 (5 points)</u></strong></h1>
Lets first get familiar with some pandas basics. Pandas is a library that handles data frames which you can think of as a python class that handles tabular data. In this section you will make a very simple function that takes in a pandas dataframe of the file attributes and classifications and returns some simple attributes. See the function skeleton and implement a count of rows, count of columns, count of rows where the classification is 1 (positive), count of rows where the classification is 0 (negative) and a percentage of classification of 1 (percent positive) in the dataset‚Äôs target column. Generally in the real world you would also use plotting tools like PowerBi, Tableau, Data Studio, Matplotlib etc to create graphics and other visuals to better understand the dataset you are working with, this step is generally known as Exploratory Data Analysis. Since we are using an autograder for this class we will skip the plotting for this project. For this task we have released a test suite on ed discussion if you are struggling to run things locally please set that up and use it to debug your function.

&nbsp;

<em>Useful Links:</em>

<ul>
<li><a href="https://pandas.pydata.org/docs/">pandas documentation ‚Äî pandas 1.5.3 documentation (pydata.org)</a></li>
<li><a href="https://www.ibm.com/topics/exploratory-data-analysis">What is Exploratory Data Analysis? | IBM</a></li>
<li><a href="https://www.kdnuggets.com/2020/05/top-10-data-visualization-tools-every-data-scientist.html">Top Data Visualization Tools | KDnuggets</a></li>
<li><a href="https://edstem.org/us/courses/30812/discussion/2695155">CS 6035 O01, OCY ‚Äì Ed Discussion (edstem.org)</a></li>
</ul>
&nbsp;

<em>Deliverables:</em>

<ul>
<li>Complete find_dataset_statistics function in task1.py</li>
<li>Submit task1.py to gradescope</li>
</ul>
&nbsp;

&nbsp;

<h1><strong><u>Task 2 (10 points)</u></strong></h1>
Now that you have a basic understanding of pandas and the dataset it is time to dive into some more complex data processing tasks. The first subtask in this task is splitting your dataset into both features and targets (columns) and splitting your dataset into training and test sets (rows). These are basic concepts in model building but at a high level it is important to hold out a subset of your data when you train a model so you can see what the expected performance is on unseen samples and so you can determine if the resulting model is overfit (performs much better on training data vs test data). Preprocessing data is important since most models only take in numerical values so categorical features need to be ‚Äúencoded‚Äù to numerical values so models can use them. Numerical scaling can be more or less useful depending on the type of model used but is especially important in linear models. These preprocessing techniques will provide you options to augment your dataset and improve model performance.

For one hot encoding and scaling functions you should return a dataframe with the encoded/scaled columns concatenated to the ‚Äòother‚Äô columns you did not transform. For the PCA functions you should return just the PCA dataframe. For the feature engineering dataframe you should return the feature engineered column attached to the input dataframe.

Example Output for one hot encoding (where color and version are encoded):

<strong>Note</strong>: For these functions (and in data science/ML in general) you should be training/fitting to the train set and predicting/transforming on the train and test sets.

<strong>Note</strong>: for onehot encoding please use the format columnName_columnValue for the new encoded column names. Ie in the sklearn example of a column named `gender` and values `male` and `female`, you would put `gender_male` and `gender_female` as your new column names after one hot encoding. If the test set has a third gender then you would denote that value you didn‚Äôt see at training time with 0‚Äôs for each encoded column.

<strong>Note</strong>: When using sklearn.preprocessing.OneHotEncoder check the output from the transformation it may cause issues if it is not a format that pandas dataframes expect. To see this and to debug it, try running your code locally (or on a google colab notebook) using the training csv with a few categorical columns.

<strong>Note</strong>: for PCA you should drop columns with na values before running fitting or transforming methods (in the real world you could also try to fill those missing values but for this project that is out of our scope)

<strong>Note</strong>: if you see the error `Test Failed: Found unknown categories` for one hot encoding make sure you check the initialization of the OneHotEncoder and make sure you are handling values in the test set that were not in the training set (we want to set those unseen values encoded as all 0s for that encoded column as we describe in the 2nd Note above)

<strong>Note</strong>: For this task we have released a test suite on ed discussion if you are struggling to run things locally please set that up and use it to debug your function.

&nbsp;

<em>Useful Links:</em>

<ul>
<li><a href="https://developers.google.com/machine-learning/crash-course/training-and-test-sets/video-lecture"><em>Training and Test Sets |&nbsp; Machine Learning&nbsp; |&nbsp; Google Developers</em></a></li>
<li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias‚Äìvariance tradeoff ‚Äì Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Overfitting">https://en.wikipedia.org/wiki/Overfitting</a></li>
<li><a href="https://365datascience.com/tutorials/statistics-tutorials/numerical-categorical-data/">Categorical and Numerical Types of Data | 365 Data Science</a></li>
<li><a href="https://scikit-learn.org/stable/index.html">scikit-learn: machine learning in Python ‚Äî scikit-learn 1.2.1 documentation</a></li>
<li><a href="https://edstem.org/us/courses/30812/discussion/2695155">CS 6035 O01, OCY ‚Äì Ed Discussion (edstem.org)</a></li>
</ul>
&nbsp;

<em>Deliverables:</em>

<ul>
<li>Make use of the scikit-learn (sklearn) python package in your function implementations</li>
<li>Complete <em>train_test_split </em>function
<ul>
<li>Using the train_test_split function from sklearn implement a function that given a dataset, target column, test size, random state and <em>True/False Value for stratify </em>will return train_features (DataFrame), test_features (DataFrame), train_targets (Series) and test_targets (Series)</li>
<li>Hint: write your code in a way that handles a case where we want to stratify vs where we don‚Äôt want to stratify (dont use stratify directly as an input to the sklearn function)</li>
</ul>
</li>
<li>Complete <em>PreprocessDataset </em>class in task2.py
<ul>
<li>one_hot_encode_columns_train
<ul>
<li>Given training features (DataFrame), one_hot_encode_cols (list of column names) and using sklearn‚Äôs OneHotEncoder. Split the data into columns that should be encoded and those that should be passed through then fit the encoder, transform the training data and create a dataframe with column names as shown above (<strong>NOTE</strong>: <em>make sure this new df uses the row indexes corresponding to the input dataframe</em>). Finally join the encoded columns with the columns from above that should be passed through. Your final results should be a Dataframe with columns in the one_hot_encode_cols list encoded and all other columns untouched.</li>
</ul>
</li>
<li>one_hot_encode_columns_test
<ul>
<li>Given training features (DataFrame), one_hot_encode_cols (list of column names) and using sklearn‚Äôs OneHotEncoder, split the data into columns that should be encoded and those that should be passed through then using the encoder fit on the training data, transform the test data and create a dataframe with column names as shown above (<strong>NOTE</strong>: <em>make sure this new df uses the row indexes corresponding to the input dataframe</em>). Finally join the encoded columns with the columns from above that should be passed through. Your final results should be a Dataframe with columns in the one_hot_encode_cols list encoded and all other columns untouched.</li>
</ul>
</li>
<li>min_max_scaled_columns_train
<ul>
<li>Given training features (DataFrame), min_max_scale_cols (list of column names) and using sklearn‚Äôs MinMaxScaler, Split the data into columns that should be encoded and those that should be passed through then fit the encoder, transform the training data and create a dataframe with column names the same as the pre-scaled feature.(<strong>NOTE</strong>: <em>make sure this new df uses the row indexes corresponding to the input dataframe</em>) Finally join the encoded columns with the columns from above that should be passed through. Your final results should be a Dataframe with columns in the min_max_scale_cols list scaled and all other columns untouched.</li>
</ul>
</li>
<li>min_max_scaled_columns_test
<ul>
<li>Given training features (DataFrame), min_max_scale_cols (list of column names) and using sklearn‚Äôs MinMaxScaler, split the data into columns that should be encoded and those that should be passed through then using the encoder fit on the training data, transform the test data and create a dataframe with column names the same as the pre-scaled feature. (<strong>NOTE</strong>:<em> make sure this new df uses the row indexes corresponding to the input dataframe</em>) Finally join the encoded columns with the columns from above that should be passed through. Your final results should be a Dataframe with columns in the min_max_scale_cols list scaled and all other columns untouched.</li>
</ul>
</li>
<li>pca_train
<ul>
<li>Given training features (DataFrame), n_components (int) and using sklearn‚Äôs PCA, initialize PCA with a random seed of 0 and n_components, train PCA on the training features dropping any columns that have NA values then transform the training set using PCA and create a DataFrame with column names component_1, component_2 .. component_n for each component you created (<strong>NOTE</strong>:<em> this new df (for the autograder this semester) should have an index from 0 to n which will not match the the row indexes corresponding to the input dataframe</em>)</li>
</ul>
</li>
<li>pca_test
<ul>
<li>Given test features (DataFrame), n_components (int) and using sklearn‚Äôs PCA, using the above trained PCA and dropping any columns that have NA values then transform the test set using PCA and create a DataFrame with column names component_1, component_2 .. component_n for each component you created (<strong>NOTE</strong>:<em> this new df (for the autograder this semester) should have an index from 0 to n which will not match the the row indexes corresponding to the input dataframe</em>)</li>
</ul>
</li>
<li>feature_engineering_train
<ul>
<li>Given training features (DataFrame) and a with the feature engineering functions passed in a dict with the format {‚Äòfeature_name‚Äô:function,} for each ‚Äòfeature_name‚Äô in the dict, create columns of name in the training DataFrame by passing the training feature dataframe to the associated function. The Returned Dataframe will consist of the input dataframe with the additional feature engineered columns from the dict (<strong>NOTE</strong>:<em> make sure this new df uses the row indexes corresponding to the input dataframe</em>)</li>
</ul>
</li>
<li>feature_engineering_test
<ul>
<li>Given test features (DataFrame) and a with the feature engineering functions passed in a dict with the format {‚Äòfeature_name‚Äô:function,} for each ‚Äòfeature_name‚Äô in the dict, create columns of name in the test DataFrame by passing the test feature dataframe to the associated function. The Returned Dataframe will consist of the input dataframe with the additional feature engineered columns from the dict (<strong>NOTE</strong>:<em> make sure this new df uses the row indexes corresponding to the input dataframe</em>)</li>
</ul>
</li>
<li>preprocess
<ul>
<li>Given a Training Features (DataFrame), Test Features (DataFrame) and the functions you created above, return Training and Test Dataframes with the one_hot_encode_cols encoded, min_max_scale_cols scaled, features described in the feature_engineering_functions engineered and any columns not affected by the above functions passed through to the output the same as they were in the input. (<strong>NOTE</strong>:<em> make sure this new df uses the row indexes corresponding to the input dataframe</em>)</li>
</ul>
</li>
<li>Submit task2.py to Gradescope</li>
</ul>
</li>
</ul>
&nbsp;

<h1><strong><u>Task 3 (15 points)</u></strong></h1>
So far we have functions to split the data and preprocessed it. Now we will run a basic model on the data to cluster files (rows) with similar attributes together. We will use an unsupervised (model with no target column) model, Kmeans, since it is simple to use and understand. Please use scikit-learn to create the model and Yellowbrick to determine the optimal value of k for our dataset.

&nbsp;

<em>Useful Links:</em>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Unsupervised_learning">Unsupervised learning ‚Äì Wikipedia</a></li>
<li><a href="https://developers.google.com/machine-learning/clustering/overview">What is Clustering? |&nbsp; Machine Learning&nbsp; |&nbsp; Google Developers</a></li>
<li><a href="https://www.geeksforgeeks.org/ml-k-means-algorithm/">ML | K-means++ Algorithm ‚Äì GeeksforGeeks</a></li>
<li><a href="https://scikit-learn.org/stable/index.html">scikit-learn: machine learning in Python ‚Äî scikit-learn 1.2.1 documentation</a></li>
<li><a href="https://www.scikit-yb.org/en/latest/">Yellowbrick: Machine Learning Visualization ‚Äî Yellowbrick v1.5 documentation (scikit-yb.org)</a></li>
</ul>
<em>Deliverables:</em>

<ul>
<li>Make use of the scikit-learn (sklearn) and yellowbrick python packages in your function implementations</li>
<li>Complete the <em>KmeansClustering </em>class in task3.py
<ul>
<li>kmeans_train
<ul>
<li>Initialize a sklearn Kmeans model using random_state, n_init =10. Initialize a yellowbrick KElbowVisualizer to search for the optimal value of k (between 1 and 10). Train the KElbowVisualizer on the training data and determine the optimal k value. Then Train a Kmeans model with the proper initialization for that optimal value of k and return the cluster ids for each row of the training set as a list.</li>
</ul>
</li>
<li>kmeans_test
<ul>
<li>Using the model you trained in the previous function return the cluster ids for each row of the test set as a list.</li>
</ul>
</li>
<li>train_add_kmeans_cluster_id_feature
<ul>
<li>Using kmeans_train add an additional column to the training features and return the training dataframe with all input features untouched and the additional cluster id column with the column name ‚Äúkmeans_cluster_id‚Äù</li>
</ul>
</li>
<li>test_add_kmeans_cluster_id_feature
<ul>
<li>Using kmeans_test add an additional column to the test features and return the test dataframe with all input features untouched and the additional cluster id column with the column name ‚Äúkmeans_cluster_id‚Äù</li>
</ul>
</li>
<li>Submit task3.py to Gradescope</li>
</ul>
</li>
</ul>
&nbsp;

<h1><strong><u>Task 4 (25 points)</u></strong></h1>
Finally we are ready to try a few different supervised classification models. We have chosen a few commonly used models for you to use here but there are many options and in the real world specific algorithms may fit a specific dataset better. You also won‚Äôt be doing any hyperparameter tuning yet to better focus on writing the code. You will train a model using the training set, predict on the training/test sets and calculate performance metrics and return a <em>ModelMetrics </em>object and trained scikit-learn model from each model function. (Note: You should use RFE for determining feature importance of the <strong>logistic regression</strong> model but do <strong>NOT </strong>use RFE for <strong>random forest</strong> or <strong>gradient boosting </strong>models to determine feature importance please use their built in values for this)

&nbsp;

Useful Links:

<ul>
<li><a href="https://developers.google.com/machine-learning/intro-to-ml/supervised">Supervised Learning |&nbsp; Machine Learning&nbsp; |&nbsp; Google Developers</a></li>
<li><a href="https://scikit-learn.org/stable/index.html">scikit-learn: machine learning in Python ‚Äî scikit-learn 1.2.1 documentation</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/classification/video-lecture">Classification |&nbsp; Machine Learning&nbsp; |&nbsp; Google Developers</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative">Classification: True vs. False and Positive vs. Negative |&nbsp; Machine Learning&nbsp; |&nbsp; Google Developers</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/classification/accuracy">Classification: Accuracy |&nbsp; Machine Learning&nbsp; |&nbsp; Google Developers</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall">Classification: Precision and Recall |&nbsp; Machine Learning&nbsp; |&nbsp; Google Developers</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">Classification: ROC Curve and AUC |&nbsp; Machine Learning&nbsp; |&nbsp; Google Developers</a></li>
</ul>
&nbsp;

Deliverables:

<ul>
<li>Make use of the scikit-learn (sklearn) python package in your function implementations</li>
<li>Complete the Following Functions in task4.py:
<ul>
<li><em>calculate_naive_metrics</em>
<ul>
<li>Given a train dataframe, test dataframe, target_col and naive assumption split out the target column from the training and test dataframes to create a feature dataframes and a target series then calculate (rounded to 4 decimal places) accuracy, recall, precision and f1 score using the sklearn functions, the train and test target values and the naive assumption.</li>
</ul>
</li>
<li><em>calculate_logistic_regression_metrics</em>
<ul>
<li>Given a train dataframe, test dataframe, target_col and logreg_kwargs split out the target column from the training and test dataframes to create a feature dataframes and a target series. Then train a logistic regression model (initialized using the kwargs) on the training data and predict (both binary predictions and probability estimates) on the training and test data. Then using those predictions and estimates along with the target values calculate (rounded to 4 decimal places) accuracy, recall, precision, f1 score, false positive rate, false negative rate and area under the reciever operator curve (using probabilities for roc auc) for both training and test datasets.</li>
<li>For Feature Importance use the top 10 features selected by RFE and sort by absolute value of the coefficient from biggest to smallest (make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col and imp_col and the index is 0-9 you can do that with `df.reset_index(drop=True)` )</li>
</ul>
</li>
<li><em>calculate_random_forest_metrics </em>
<ul>
<li>Given a train dataframe, test dataframe, target_col and rf_kwargs split out the target column from the training and test dataframes to create a feature dataframes and a target series. Then train a random forest model (initialized using the kwargs) on the training data and predict (both binary predictions and probability estimates) on the training and test data. Then using those predictions and estimates along with the target values calculate (rounded to 4 decimal places) accuracy, recall, precision, f1 score, false positive rate, false negative rate and area under the reciever operator curve (using probabilities for roc auc)&nbsp; for both training and test datasets</li>
<li>For Feature Importance use the top 10 features using the built in feature importance attributes as sorted from biggest to smallest (make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col and imp_col and the index is 0-9 you can do that with `df.reset_index(drop=True)` )</li>
</ul>
</li>
<li><em>calculate_gradient_boosting_metrics </em>
<ul>
<li>Given a train dataframe, test dataframe, target_col and gb_kwargs split out the target column from the training and test dataframes to create a feature dataframes and a target series. Then train a gradient boosting model (initialized using the kwargs) on the training data and predict (both binary predictions and probability estimates) on the training and test data. Then using those predictions and estimates along with the target values calculate (rounded to 4 decimal places) accuracy, recall, precision, f1 score, false positive rate, false negative rate and area under the reciever operator curve (using probabilities for roc auc)&nbsp; for both training and test datasets</li>
<li>For Feature Importance use the top 10 features using the built in feature importance attributes as sorted from biggest to smallest (make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col and imp_col and the index is 0-9 you can do that with `df.reset_index(drop=True)` )</li>
</ul>
</li>
<li>Submit task4.py to Gradescope</li>
</ul>
</li>
</ul>
&nbsp;

&nbsp;

<h1><strong><u>Task 5 (25 points)</u></strong></h1>
Now that you have written functions for different steps of the model building process you will put it all together. You will write code that trains a model with hyperparameters you determine (you should do any tuning locally or in a notebook ie don‚Äôt tune your model in gradescope since the autograder will likely timeout). It will take in the CLAMP training data, train a model then predict on a test set and output values from 0 to 1 for each row and our autograder will compare your predictions with the correct answers and to get credit you will need a roc auc score of .9 or higher on the test set (should not require much hyperparameter tuning for this dataset). This is basically a simulation of how your model would perform in the ‚Äúproduction‚Äù system using batch inference.

&nbsp;

&nbsp;

Deliverables:

<ul>
<li>Make use of any of the techniques we covered in this project to train a model and return predicted probabilities for each row of the test set as a DataFrame with columns index (same as your index from the input test df) and malware_score (predicted probabilities).</li>
<li>Complete the <em>train_model_return_scores </em>function in task5.py</li>
<li>Submit task5.py to Gradescope</li>
</ul>
